Ceph install note : Ceph luminous with OpenStack Mitaka 

- Install ceph (Ceph install guide)
- Format xfs volume (if needed): mkfs.xfs /dev/...
- Create lvm volume if used:
	+ Creating Physical Volume : sudo vgcreate volume_group_name /dev/sda
	+ Creating a Logical Volume From All Remaining Free Space : sudo lvcreate -l 100%FREE -n test2 LVMVolGroup
- Remove lvm volume is used by another device : dmsetup 

- Create OSD : ceph-deploy osd create --data /dev/sda4 svim-53-191
			Note If you are creating an OSD on an LVM volume or loopback volume, 
			the argument to --data must be volume_group/lv_name, rather than the path to the volume’s block device.
			
- Error : RuntimeError: Cannot use device (/dev/loop0). A vg/lv path or an existing device is needed  (error when using loopback volume without create lvm volume from this loopback volume)
  Solution : Create lvm volume from this loopback volume : 
					pvcreate /dev/loop0
					sudo vgcreate volume_group_ceph /dev/loop0
					sudo lvcreate -l 100%FREE -n lv_ceph volume_group_ceph
           		Then create osd : ceph-deploy osd create --data volume_group_ceph/lv_ceph ceph-node-20
				
				Notice : After reboot server, loopback volume will be removed, run this command to restore or add to rc.local : 
				losetup --find --show /loopback.img
- Error : HEALTH_WARN clock skew detected on mon.ceph2 Monitor clock skew detected
  Solution:
    + sudo service ntp stop
    + sudo ntpdate ntp.ubuntu.com
    + sudo ntpd -gq
	+ sudo service ntp start
	+ sudo  restart ceph-mon-all

- Adding a monitor : ceph-deploy mon create svim-53-191
- Truong hop tao them 1 cluster moi chua 1 node moi, sau do muon join node moi do vao cluster hien tai thi can phai move thu muc /var/lib/ceph/mon tren
  node moi vao /tmp sau do chay command "ceph-deploy mon create " tren node admin de add node moi vao mon cua cluster cu
- Ceph commands:
	+ Remove pools: # ceph tell mon.* injectargs --mon-allow-pool-delete=true
					# ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
					# ceph tell mon.* injectargs --mon-allow-pool-delete=false
	+ Remove osd : 	ceph osd crush remove osd.<id>
					ceph auth del osd.<id>
					ceph osd rm <id>
- Config Ceph with Openstack Glance:
	+ ceph osd pool create images 128
	+ rbd pool init volumes
	
	
- Config Ceph with OpenStack Cinder:
	+ ceph osd pool create volumes 128
	+ rbd pool init volumes
	+ ssh <cinder-node> sudo tee /etc/ceph/ceph.conf </etc/ceph/ceph.conf --> bỏ qua vì là compute và cinder node nên cài rồi
	+ sudo apt-get install ceph-common -> bỏ qua vì là ceph node nên cài rồi
	+ ceph auth get-or-create client.cinder mon 'profile rbd' osd 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'
	+ ceph auth get-or-create client.cinder | ssh {your-volume-server} sudo tee /etc/ceph/ceph.client.cinder.keyring
	  ssh {your-cinder-volume-server} sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring
	+ ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key
	+ uuidgen
		457eb676-33da-42ec-9a8c-9293d545c337 --> save to config cinder and nova

		cat > secret.xml <<EOF
		<secret ephemeral='no' private='no'>
		  <uuid>457eb676-33da-42ec-9a8c-9293d545c337</uuid>
		  <usage type='ceph'>
			<name>client.cinder secret</name>
		  </usage>
		</secret>
		EOF
		sudo virsh secret-define --file secret.xml
		Secret 457eb676-33da-42ec-9a8c-9293d545c337 created
		sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) && rm client.cinder.key secret.xml
	+ Config cinder : /etc/cinder/cinder.conf
		[DEFAULT]
			...
			enabled_backends = ceph
			...
			[ceph]
			volume_driver = cinder.volume.drivers.rbd.RBDDriver
			volume_backend_name = ceph
			rbd_pool = volumes
			rbd_ceph_conf = /etc/ceph/ceph.conf
			rbd_flatten_volume_from_snapshot = false
			rbd_max_clone_depth = 5
			rbd_store_chunk_size = 4
			rados_connect_timeout = -1
			glance_api_version = 2  --> multiple backend
	
	+ Config nova to attach volume : /etc/nova/nova.conf
		[libvirt]
			rbd_user = cinder
			rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337
	+ Restart services : 
		sudo service nova-compute restart
		sudo service cinder-volume restart
	
	+ Tạo volume type : 
		$ openstack admin volume type create ceph
		$ openstack volume type set ceph \
		  --property volume_backend_name=ceph
		$ openstack volume type list --long

- List cinder volumes in ceph 
	+ sudo rbd ls volumes
	+ sudo rbd info volumes/<volume-id>
	
- When 1 osd failed : 
	+ sudo systemctl start ceph-osd@<id>
	+ sudo systemctl start ceph.target
	
- When using ceph with openstack, step copy config ceph to openstack node : 
cephuser@ceph-admin:~/ceph-cluster$ ssh scloud@20.0.1.15 sudo tee /etc/ceph/ceph.conf </etc/ceph/ceph.conf
scloud@20.0.1.15's password:
sudo: no tty present and no askpass program specified
===> create ceph user on openstack nodes and add it to sudoer file : 
 useradd -d /home/cephuser -m cephuser
 passwd cephuser

 echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser
 chmod 0440 /etc/sudoers.d/cephuser

- Ceph command : 
1. Check or watch cluster health: ceph status || ceph -w
If you want to quickly verify that your cluster is operating normally, use ceph status to get a birds-eye view of cluster status (hint: typically, you want your cluster to be active + clean). You can also watch cluster activity in real-time with ceph -w; you'll typically use this when you add or remove OSDs and want to see the placement groups adjust.

2. Check cluster usage stats: ceph df
To check a cluster’s data usage and data distribution among pools, use ceph df. This provides information on available and used storage space, plus a list of pools and how much storage each pool consumes. Use this often to check that your cluster is not running out of space.

3. Check placement group stats: ceph pg dump
When you need statistics for the placement groups in your cluster, use ceph pg dump. You can get the data in JSON as well in case you want to use it for automatic report generation.

4. View the CRUSH map: ceph osd tree
Need to troubleshoot a cluster by identifying the physical data center, room, row and rack of a failed OSD faster? Use ceph osd tree, which produces an ASCII art CRUSH tree map with a host, its OSDs, whether they are up and their weight.

5. Create or remove OSDs: ceph osd create || ceph osd rm
Use ceph osd create to add a new OSD to the cluster. If no UUID is given, it will be set automatically when the OSD starts up. When you need to remove an OSD from the CRUSH map, use ceph osd rm with the UUID.

6. Create or delete a storage pool: ceph osd pool create || ceph osd pool delete
Create a new storage pool with a name and number of placement groups with ceph osd pool create. Remove it (and wave bye-bye to all the data in it) with ceph osd pool delete.

7. Repair an OSD: ceph osd repair
Ceph is a self-repairing cluster. Tell Ceph to attempt repair of an OSD by calling ceph osd repair with the OSD identifier.

8. Benchmark an OSD: ceph tell osd. bench*
Added an awesome new storage device to your cluster? Use ceph tell to see how well it performs by running a simple throughput benchmark. By default, the test writes 1 GB in total in 4-MB increments.

9. Adjust an OSD’s crush weight: ceph osd crush reweight
Ideally, you want all your OSDs to be the same in terms of thoroughput and capacity...but this isn't always possible. When your OSDs differ in their key attributes, use ceph osd crush reweight to modify their weights in the CRUSH map so that the cluster is properly balanced and OSDs of different types receive an appropriately-adjusted number of I/O requests and data.

10. List cluster keys: ceph auth list
Ceph uses keyrings to store one or more Ceph authentication keys and capability specifications. The ceph auth list command provides an easy way to to keep track of keys and capabilities
